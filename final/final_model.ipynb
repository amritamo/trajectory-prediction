{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbyCs7Q6nBLy"
      },
      "source": [
        "# CSE251B Project Final Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_byyxmunBLz"
      },
      "source": [
        "## Imports/Initializations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "ubP08xYFnPCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJhX3IxWnBL0"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT FORGET TO UPLOAD kaggle.json FILE"
      ],
      "metadata": {
        "id": "idz1Zr3APhbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle competitions download cse-251-b-2025\n",
        "\n",
        "!unzip cse-251-b-2025.zip\n",
        "\n",
        "train_npz = np.load('./train.npz')\n",
        "train_data = train_npz['data']\n",
        "test_npz  = np.load('./test_input.npz')\n",
        "test_data  = test_npz['data']\n",
        "\n",
        "print(train_data.shape, test_data.shape)\n",
        "\n",
        "# Split once for later use\n",
        "X_train = train_data[..., :50, :]\n",
        "Y_train = train_data[:, 0, 50:, :2]"
      ],
      "metadata": {
        "id": "bbKhm0ER44zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdRs0lUtnBL1"
      },
      "source": [
        "## Dataset Initializations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetTrain(Dataset):\n",
        "    def __init__(self, data, scale=10.0, augment=True):\n",
        "      \"\"\"\n",
        "      Input(s):\n",
        "      data (np.ndarray: N, 50, 110, 6): training data\n",
        "      scale (float): scale factor for norm\n",
        "      augment (bool): Toggle augmentation\n",
        "\n",
        "      Output(s):\n",
        "      self (struct): Internal struct with data, scale, and augmentation value\n",
        "      \"\"\"\n",
        "      self.data = data\n",
        "      self.scale = scale\n",
        "      self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "      \"\"\"\n",
        "      Get length of dataset\n",
        "\n",
        "      Output:\n",
        "      length (int): length of dataset\n",
        "      \"\"\"\n",
        "      return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      \"\"\"\n",
        "      Get an item from the dataset\n",
        "\n",
        "      Input(s):\n",
        "      idx (int): index of dataset item\n",
        "\n",
        "      Output(s):\n",
        "      datapoint (torch_geometric.data.Data): dataset item\n",
        "      \"\"\"\n",
        "      # Extract scene and get the history and future timesteps (50 historical, 60 future)\n",
        "      scene = self.data[idx]\n",
        "      historical_traj = scene[:, :50, :].copy()\n",
        "      future_traj = torch.tensor(scene[0, 50:, :2].copy(), dtype=torch.float32)\n",
        "\n",
        "      # If the augmentation toggle is True, rotate and flip the data\n",
        "      if self.augment:\n",
        "        if np.random.rand() < 0.75:\n",
        "          # Random heading\n",
        "          theta = np.random.uniform(-np.pi, np.pi)\n",
        "          R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta),  np.cos(theta)]], dtype=np.float32)\n",
        "\n",
        "          # Rotate the trajectory\n",
        "          historical_traj[..., :2] = historical_traj[..., :2] @ R\n",
        "          historical_traj[..., 2:4] = historical_traj[..., 2:4] @ R\n",
        "          future_traj = future_traj @ R\n",
        "        if np.random.rand() < 0.5:\n",
        "          # Flip trajectory\n",
        "          historical_traj[..., 0] *= -1\n",
        "          historical_traj[..., 2] *= -1\n",
        "          future_traj[:, 0] *= -1\n",
        "\n",
        "      # Isolate ego position, heading, and velocity in 50th time step and use as origin\n",
        "      pos_origin = historical_traj[0, 49, :2].copy()\n",
        "      heading_origin = historical_traj[0, 49, 4].copy()\n",
        "      velocity_origin = historical_traj[0, 49, 2:4].copy()\n",
        "\n",
        "      # Normalize the trajectory position\n",
        "      historical_traj[..., :2] = historical_traj[..., :2] - pos_origin\n",
        "      future_traj = future_traj - pos_origin\n",
        "\n",
        "      # Normalize heading\n",
        "      historical_traj[..., 4] = historical_traj[..., 4] - heading_origin\n",
        "\n",
        "      # Normalize velocity\n",
        "      historical_traj[..., 2:4] = historical_traj[..., 2:4] - velocity_origin\n",
        "\n",
        "      # Apply scale to trajectory data\n",
        "      historical_traj[..., :4] = historical_traj[..., :4] / self.scale\n",
        "      future_traj = future_traj / self.scale\n",
        "\n",
        "      # Use a validity mask and append to data to help model know which objects are zero padding and which are valid\n",
        "      validity_mask = torch.any(torch.tensor(historical_traj, dtype=torch.float32) != 0, dim=(1, 2))\n",
        "      validity_mask = validity_mask.float()\n",
        "\n",
        "      # Output the data item in form (x, y, pos_origin, scale, heading_origin, velocity_origin, validity_mask)\n",
        "      data_item = Data(\n",
        "        x=torch.tensor(historical_traj, dtype=torch.float32),\n",
        "        y=future_traj.type(torch.float32),\n",
        "        pos_origin=torch.tensor(pos_origin, dtype=torch.float32).unsqueeze(0),\n",
        "        scale=torch.tensor(self.scale, dtype=torch.float32),\n",
        "        heading_origin=torch.tensor(heading_origin, dtype=torch.float32),\n",
        "        velocity_origin=torch.tensor(velocity_origin, dtype=torch.float32),\n",
        "        validity_mask=validity_mask,\n",
        "      )\n",
        "\n",
        "      return data_item\n",
        "\n",
        "\n",
        "class DatasetTest(Dataset):\n",
        "  def __init__(self, data, scale=10.0):\n",
        "    \"\"\"\n",
        "    Input(s):\n",
        "    data (np.ndarray: N, 50, 50, 6): testing data\n",
        "    scale (float): scale factor for norm\n",
        "\n",
        "    Output(s):\n",
        "    self (struct): Internal struct with data and scale value\n",
        "    \"\"\"\n",
        "    self.data = data\n",
        "    self.scale = scale\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Get length of dataset\n",
        "\n",
        "    Output:\n",
        "    length (int): length of dataset\n",
        "    \"\"\"\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Get an item from the test dataset\n",
        "\n",
        "    Input(s):\n",
        "    idx (int): index of dataset item\n",
        "\n",
        "    Output(s):\n",
        "    datapoint (torch_geometric.data.Data): test dataset item\n",
        "    \"\"\"\n",
        "    # Extract scene (only has historical data so 50 timesteps)\n",
        "    scene = self.data[idx]\n",
        "    historical_traj = scene.copy()\n",
        "\n",
        "    # Isolate ego position, heading, and velocity in 50th time step and use as origin\n",
        "    pos_origin = historical_traj[0, 49, :2].copy()\n",
        "    heading_origin = historical_traj[0, 49, 4].copy()\n",
        "    velocity_origin = historical_traj[0, 49, 2:4].copy()\n",
        "\n",
        "    # Normalize position\n",
        "    historical_traj[..., :2] = historical_traj[..., :2] - pos_origin\n",
        "\n",
        "    # Normalize heading\n",
        "    historical_traj[..., 4] = historical_traj[..., 4] - heading_origin\n",
        "\n",
        "    # Normalize velocity\n",
        "    historical_traj[..., 2:4] = historical_traj[..., 2:4] - velocity_origin\n",
        "\n",
        "    # Scale trajectory\n",
        "    historical_traj[..., :4] = historical_traj[..., :4] / self.scale\n",
        "\n",
        "    # Use a validity mask and append to data to help model know which objects are zero padding and which are valid\n",
        "    validity_mask = torch.any(torch.tensor(historical_traj, dtype=torch.float32) != 0, dim=(1, 2))\n",
        "    validity_mask = validity_mask.float()\n",
        "\n",
        "    # Output the data item in form (x, pos_origin, scale, heading_origin, velocity_origin, validity mask)\n",
        "    data_item = Data(\n",
        "        x=torch.tensor(historical_traj, dtype=torch.float32),\n",
        "        pos_origin=torch.tensor(pos_origin, dtype=torch.float32).unsqueeze(0),\n",
        "        scale=torch.tensor(self.scale, dtype=torch.float32),\n",
        "        heading_origin=torch.tensor(heading_origin, dtype=torch.float32),\n",
        "        velocity_origin=torch.tensor(velocity_origin, dtype=torch.float32),\n",
        "        validity_mask=validity_mask,\n",
        "    )\n",
        "\n",
        "    return data_item"
      ],
      "metadata": {
        "id": "WHNjog4wmLA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvHsam1snBL2"
      },
      "source": [
        "## Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len=50):\n",
        "    \"\"\"\n",
        "    Initialize positional encoder\n",
        "\n",
        "    Input(s):\n",
        "    d_model (int): embedding dimension\n",
        "    max_len (int): max sequence length\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Init positional encoder struct\n",
        "    pos_enc = torch.zeros(max_len, d_model)\n",
        "\n",
        "    # Positional vector\n",
        "    pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    # Scale factor\n",
        "    scale_val = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "    # Even columns have sin values, odd columns have cos values for positions\n",
        "    pos_enc[:, 0::2] = torch.sin(pos * scale_val)\n",
        "    pos_enc[:, 1::2] = torch.cos(pos * scale_val)\n",
        "\n",
        "    # Positional encoding stored as non-param value in struct\n",
        "    self.register_buffer('pos_enc', pos_enc.unsqueeze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass of positional encoder\n",
        "\n",
        "    Input(s):\n",
        "    x (torch.Tensor): input tensor\n",
        "\n",
        "    Output(s):\n",
        "    x (torch.Tensor): output tensor\n",
        "    \"\"\"\n",
        "    return x + self.pos_enc[:, :x.size(1), :]\n",
        "\n",
        "class TrajTransformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               num_agents=50,\n",
        "               time_steps=50,\n",
        "               feature_dim=6,\n",
        "               d_model=192,\n",
        "               nhead=8,\n",
        "               num_layers=3,\n",
        "               dropout=0.05,\n",
        "               future_steps=60):\n",
        "    \"\"\"\n",
        "    Initialize transformer model\n",
        "\n",
        "    Input(s):\n",
        "    num_agents (int): number of agents (sequence length)\n",
        "    time_steps (int): historical trajectory per agent\n",
        "    feature_dim (int): input features (x, y, v_x, v_y, heading, validity_mask)\n",
        "    d_model (int): embedding dimension\n",
        "    nhead (int): attention heads\n",
        "    num_layers (int): number of model layers\n",
        "    dropout (float): dropout value (% drop)\n",
        "    future_steps (int): future trajectories to predict\n",
        "\n",
        "    Output(s):\n",
        "    self (struct): Internal struct with model parameters\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Assign the input values to the internal struct\n",
        "    self.num_agents = num_agents\n",
        "    self.time_steps = time_steps\n",
        "    self.feature_dim = feature_dim\n",
        "    self.d_model = d_model\n",
        "    self.future_steps = future_steps\n",
        "\n",
        "    # Process agent's history into fixed vector\n",
        "    self.temporal_encoder = nn.Sequential(\n",
        "        nn.Linear(time_steps * feature_dim, d_model),\n",
        "        nn.GELU(),\n",
        "        nn.LayerNorm(d_model),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "    # Positional encoding for agent sequence\n",
        "    self.pos_encoder = PositionalEncoding(d_model, max_len=num_agents)\n",
        "\n",
        "    # Init the transformer encoder layer\n",
        "    encoder_layer = nn.TransformerEncoderLayer(\n",
        "        d_model=d_model,\n",
        "        nhead=nhead,\n",
        "        dim_feedforward=d_model * 2,\n",
        "        dropout=dropout,\n",
        "        activation='gelu',\n",
        "        batch_first=True,\n",
        "        norm_first=True\n",
        "    )\n",
        "    self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "    # Output only ego agent position at 110th timestep\n",
        "    self.output_projection = nn.Sequential(\n",
        "        nn.Linear(d_model, d_model // 2),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(d_model // 2, future_steps * 2)\n",
        "    )\n",
        "\n",
        "    # Initialize weights\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    \"\"\"\n",
        "    Initialize weights\n",
        "\n",
        "    Input(s):\n",
        "    self (struct): internal structure of transformer\n",
        "    module (torch.nn.Module): module to initialize\n",
        "    \"\"\"\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.xavier_uniform_(module.weight)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.LayerNorm):\n",
        "      torch.nn.init.ones_(module.weight)\n",
        "      torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "  def forward(self, data_batch):\n",
        "    \"\"\"\n",
        "    Forward pass for transformer model\n",
        "\n",
        "    Input(s):\n",
        "    data_batch (torch_geometric.data.Data): input data\n",
        "\n",
        "    Output(s):\n",
        "    output (torch.Tensor): output tensor\n",
        "    \"\"\"\n",
        "    # Extract input and validity mask\n",
        "    x = data_batch.x\n",
        "    validity_mask = data_batch.validity_mask\n",
        "\n",
        "    # Reshape input to (batch_size, num_agents, time_steps * feature_dim)\n",
        "    if x.dim() == 3:\n",
        "      batch_size = x.size(0) // self.num_agents\n",
        "      x = x.view(batch_size, self.num_agents, self.time_steps, self.feature_dim)\n",
        "\n",
        "    # Get batch size\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # Flatten dimension for each agent\n",
        "    x_flat = x.view(batch_size, self.num_agents, -1)\n",
        "\n",
        "    # Encode features for each agent (batch_size, num_agents, d_model)\n",
        "    tokens = self.temporal_encoder(x_flat)\n",
        "\n",
        "    # Add positional encoding\n",
        "    tokens = self.pos_encoder(tokens)\n",
        "\n",
        "    # Create attention mask from validity_mask\n",
        "    if validity_mask.dim() == 1:\n",
        "      validity_mask = validity_mask.view(batch_size, self.num_agents)\n",
        "\n",
        "    # Apply transformer\n",
        "    encoded = self.transformer(tokens, src_key_padding_mask=~validity_mask.bool())\n",
        "\n",
        "    # Extract ego features (batch_size, d_model)\n",
        "    ego_features = encoded[:, 0, :]\n",
        "\n",
        "    # Generate future trajectory\n",
        "    output = self.output_projection(ego_features)\n",
        "    output = output.view(batch_size, self.future_steps, 2)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "xVdt6r7VDBwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCUIdiCRnBL2"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device for training speedup\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print(\"Using Apple Silicon GPU\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"Using CUDA GPU\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Set scale\n",
        "scale = 3.0\n",
        "\n",
        "# Set seeds\n",
        "torch.manual_seed(251)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Split data into training and validation dataset\n",
        "N = len(train_data)\n",
        "val_size = int(0.1 * N)\n",
        "train_size = N - val_size\n",
        "\n",
        "train_dataset = DatasetTrain(train_data[:train_size], scale=scale, augment=True)\n",
        "val_dataset = DatasetTrain(train_data[train_size:], scale=scale, augment=False)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: Batch.from_data_list(x))\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: Batch.from_data_list(x))\n",
        "\n",
        "# Initialize model\n",
        "model = TrajTransformer().to(device)\n",
        "\n",
        "# Initialize training objects\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.25)\n",
        "early_stopping_patience = 15\n",
        "best_val_loss = float(\"inf\")\n",
        "no_improvement = 0\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Loop for 166 epochs\n",
        "for epoch in tqdm.tqdm(range(166), desc=\"Epoch\", unit=\"epoch\"):\n",
        "  # Put model in training mode\n",
        "  model.train()\n",
        "\n",
        "  # Initialize loss\n",
        "  train_loss = 0\n",
        "\n",
        "  # Predict on the batch and calculate loss\n",
        "  for batch in train_dataloader:\n",
        "    batch = batch.to(device)\n",
        "    pred = model(batch)\n",
        "    y = batch.y.view(batch.num_graphs, 60, 2)\n",
        "    loss = criterion(pred, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Initialize validation metrics\n",
        "  val_loss = 0\n",
        "  val_mae = 0\n",
        "  val_mse = 0\n",
        "\n",
        "  # Predict on validation set and evaluate validation MSE and MAE\n",
        "  with torch.no_grad():\n",
        "      for batch in val_dataloader:\n",
        "          batch = batch.to(device)\n",
        "          pred = model(batch)\n",
        "          y = batch.y.view(batch.num_graphs, 60, 2)\n",
        "          val_loss += criterion(pred, y).item()\n",
        "\n",
        "          # Denormalize\n",
        "          pred = pred * batch.scale.view(-1, 1, 1) + batch.pos_origin.unsqueeze(1)\n",
        "          y = y * batch.scale.view(-1, 1, 1) + batch.pos_origin.unsqueeze(1)\n",
        "          val_mae += nn.L1Loss()(pred, y).item()\n",
        "          val_mse += nn.MSELoss()(pred, y).item()\n",
        "\n",
        "  # Average loss and metrics over batch size\n",
        "  train_loss /= len(train_dataloader)\n",
        "  val_loss /= len(val_dataloader)\n",
        "  val_mae /= len(val_dataloader)\n",
        "  val_mse /= len(val_dataloader)\n",
        "\n",
        "  # Call scheduler to potentially adjust learning rate\n",
        "  scheduler.step()\n",
        "\n",
        "  # Console output for visibility\n",
        "  tqdm.tqdm.write(f\"Epoch {epoch:03d} | LR {optimizer.param_groups[0]['lr']:.6f} | Train MSE {train_loss:.4f} | Val MSE {val_loss:.4f} | MAE {val_mae:.4f} | MSE {val_mse:.4f}\"\"\")\n",
        "\n",
        "  # Check if model needs to be saved and if early stopping should be triggered\n",
        "  if val_loss < best_val_loss - 1e-3:\n",
        "    best_val_loss = val_loss\n",
        "    no_improvement = 0\n",
        "    torch.save(model.state_dict(), f\"best_model.pt\")\n",
        "  else:\n",
        "    no_improvement += 1\n",
        "    if no_improvement >= early_stopping_patience:\n",
        "      print(\"Early stopping triggered.\")\n",
        "      break"
      ],
      "metadata": {
        "id": "ysv6sqbA_Vr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMc4AoTunBL3"
      },
      "source": [
        "## Testing / Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBgQ9ncjnBL3"
      },
      "outputs": [],
      "source": [
        "def plot_trajectory(ax, pred, gt, title=None):\n",
        "    ax.cla()\n",
        "    # Plot the predicted future trajectory\n",
        "    ax.plot(pred[0,:60,0], pred[0,:60,1], color='red', label='Predicted Future Trajectory')\n",
        "\n",
        "    # Plot the ground truth future trajectory\n",
        "    ax.plot(gt[0,:60,0], gt[0,:60,1], color='blue', label='Ground Truth Future Trajectory')\n",
        "\n",
        "    # Optionally set axis limits, labels, and title.\n",
        "    x_max = max(pred[..., 0].max(), gt[..., 0].max())\n",
        "    x_min = min(pred[..., 0].min(), gt[..., 0].min())\n",
        "    y_max = max(pred[..., 1].max(), gt[..., 1].max())\n",
        "    y_min = min(pred[..., 1].min(), gt[..., 1].min())\n",
        "\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.set_xlabel('X-axis')\n",
        "    ax.set_ylabel('Y-axis')\n",
        "\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "\n",
        "    ax.legend()\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYo3CaACnBL3"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"/content/best_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# randomly select 4 samples from the validation set\n",
        "random_indices = random.sample(range(len(val_dataset)), 4)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 10))\n",
        "axes = axes.flatten()  # Flatten the array to iterate single axes objects\n",
        "\n",
        "for i, idx in enumerate(random_indices):\n",
        "  batch = val_dataset[idx]\n",
        "  batch = batch.to(device)\n",
        "  pred = model(batch)\n",
        "  gt = torch.stack(torch.split(batch.y, 60, dim=0), dim=0)\n",
        "\n",
        "  pred = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
        "  gt = torch.stack(torch.split(batch.y, 60, dim=0), dim=0) * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
        "\n",
        "  pred = pred.detach().cpu().numpy()\n",
        "  gt = gt.detach().cpu().numpy()\n",
        "\n",
        "  # Plot the trajectory using the i-th axis\n",
        "  plot_trajectory(axes[i], pred, gt, title=f\"Sample {idx}\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4N8IdtFnBL3"
      },
      "source": [
        "## Export Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SORS2p6PnBL3"
      },
      "outputs": [],
      "source": [
        "test_dataset = DatasetTest(test_data, scale=scale)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
        "                         collate_fn=lambda xs: Batch.from_data_list(xs))\n",
        "\n",
        "best_model = torch.load(\"/content/best_model.pt\")\n",
        "model = TrajTransformer().to(device)\n",
        "\n",
        "model.load_state_dict(best_model)\n",
        "model.eval()\n",
        "\n",
        "pred_list = []\n",
        "with torch.no_grad():\n",
        "  for batch in test_loader:\n",
        "    batch = batch.to(device)\n",
        "    pred_norm = model(batch)\n",
        "\n",
        "    # Reshape the prediction to (N, 60, 2)\n",
        "    pred = pred_norm * batch.scale.view(-1,1,1) + batch.origin.unsqueeze(1)\n",
        "    pred_list.append(pred.cpu().numpy())\n",
        "pred_list = np.concatenate(pred_list, axis=0)  # (N,60,2)\n",
        "pred_output = pred_list.reshape(-1, 2)  # (N*60, 2)\n",
        "output_df = pd.DataFrame(pred_output, columns=['x', 'y'])\n",
        "output_df.index.name = 'index'\n",
        "output_df.to_csv('submission.csv', index=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}