{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75846628",
   "metadata": {},
   "source": [
    "# Initialize Packages and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c946e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef40725",
   "metadata": {},
   "source": [
    "## Load data from Kaggle (must upload kaggle.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98763be",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle competitions download cse-251-b-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip cse-251-b-2025.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_npz = np.load('./train.npz')\n",
    "train_data = train_npz['data']\n",
    "test_npz  = np.load('./test_input.npz')\n",
    "test_data  = test_npz['data']\n",
    "\n",
    "print(train_data.shape, test_data.shape)\n",
    "\n",
    "# Split once for later use\n",
    "X_train = train_data[..., :50, :]\n",
    "Y_train = train_data[:, 0, 50:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5728565",
   "metadata": {},
   "source": [
    "# Dataset Structure Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee861ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDatasetTrain(Dataset):\n",
    "    def __init__(self, data, scale=10.0, augment=True):\n",
    "        \"\"\"\n",
    "        data: Shape (N, 50, 110, 6) Training data\n",
    "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
    "        augment: Whether to apply data augmentation (only for training)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx]\n",
    "        # Getting 50 historical timestamps and 60 future timestamps\n",
    "        hist = scene[:, :50, :].copy()    # (agents=50, time_seq=50, 6)\n",
    "        future = torch.tensor(scene[0, 50:, :2].copy(), dtype=torch.float32)  # (60, 2)\n",
    "\n",
    "        # Data augmentation(only for training)\n",
    "        if self.augment:\n",
    "            if np.random.rand() < 0.5:\n",
    "                theta = np.random.uniform(-np.pi, np.pi)\n",
    "                R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                              [np.sin(theta),  np.cos(theta)]], dtype=np.float32)\n",
    "                # Rotate the historical trajectory and future trajectory\n",
    "                hist[..., :2] = hist[..., :2] @ R\n",
    "                hist[..., 2:4] = hist[..., 2:4] @ R\n",
    "                future = future @ R\n",
    "            if np.random.rand() < 0.5:\n",
    "                hist[..., 0] *= -1\n",
    "                hist[..., 2] *= -1\n",
    "                future[:, 0] *= -1\n",
    "\n",
    "        # Use the last timeframe of the historical trajectory as the origin\n",
    "        origin = hist[0, 49, :2].copy()  # (2,)\n",
    "        hist[..., :2] = hist[..., :2] - origin\n",
    "        future = future - origin\n",
    "\n",
    "        # Normalize the historical trajectory and future trajectory\n",
    "        hist[..., :4] = hist[..., :4] / self.scale\n",
    "        future = future / self.scale\n",
    "\n",
    "        data_item = Data(\n",
    "            x=torch.tensor(hist, dtype=torch.float32),\n",
    "            y=future.type(torch.float32),\n",
    "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n",
    "            scale=torch.tensor(self.scale, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "        return data_item\n",
    "\n",
    "\n",
    "class TrajectoryDatasetTest(Dataset):\n",
    "    def __init__(self, data, scale=10.0):\n",
    "        \"\"\"\n",
    "        data: Shape (N, 50, 110, 6) Testing data\n",
    "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Testing data only contains historical trajectory\n",
    "        scene = self.data[idx]  # (50, 50, 6)\n",
    "        hist = scene.copy()\n",
    "\n",
    "        origin = hist[0, 49, :2].copy()\n",
    "        hist[..., :2] = hist[..., :2] - origin\n",
    "        hist[..., :4] = hist[..., :4] / self.scale\n",
    "\n",
    "        data_item = Data(\n",
    "            x=torch.tensor(hist, dtype=torch.float32),\n",
    "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n",
    "            scale=torch.tensor(self.scale, dtype=torch.float32),\n",
    "        )\n",
    "        return data_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c016eb7f",
   "metadata": {},
   "source": [
    "# Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d4f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=128, output_dim=60 * 2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x= x.reshape(-1, 50, 50, 6)  # (batch_size, num_agents, seq_len, input_dim)\n",
    "        x = x[:, 0, :, :] # Only Consider ego agent index 0\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out is of shape (batch_size, seq_len, hidden_dim) and we want the last time step output\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out.view(-1, 60, 2)\n",
    "\n",
    "class LSTM_Dropout(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=128, output_dim=60 * 2, dropout_rate=0.2):\n",
    "        super(LSTM_Dropout, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = x.reshape(-1, 50, 50, 6)  # (batch_size, num_agents, seq_len, input_dim)\n",
    "        x = x[:, 0, :, :] # Only Consider ego agent index 0\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out is of shape (batch_size, seq_len, hidden_dim) and we want the last time step output\n",
    "        final_output = self.dropout(lstm_out[:, -1, :])\n",
    "        out = self.fc(final_output)\n",
    "        return out.view(-1, 60, 2)\n",
    "\n",
    "class LSTM_DropoutNorm(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=128, output_dim=60 * 2, dropout_rate=0.2):\n",
    "        super(LSTM_DropoutNorm, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, dropout=dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x= x.reshape(-1, 50, 50, 6)  # (batch_size, num_agents, seq_len, input_dim)\n",
    "        x = x[:, 0, :, :] # Only Consider ego agent index 0\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out is of shape (batch_size, seq_len, hidden_dim) and we want the last time step output\n",
    "        final_output = lstm_out[:, -1, :]\n",
    "        normalized = self.layer_norm(final_output)\n",
    "        regularized = self.dropout(normalized)\n",
    "        out = self.fc(regularized)\n",
    "        return out.view(-1, 60, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5152c3",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b671d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize dataset and check device\n",
    "\n",
    "torch.manual_seed(251)\n",
    "np.random.seed(42)\n",
    "\n",
    "scale = 3.0\n",
    "\n",
    "N = len(train_data)\n",
    "val_size = int(0.1 * N)\n",
    "train_size = N - val_size\n",
    "\n",
    "train_dataset = TrajectoryDatasetTrain(train_data[:train_size], scale=scale, augment=True)\n",
    "val_dataset = TrajectoryDatasetTrain(train_data[train_size:], scale=scale, augment=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "\n",
    "# Set device for training speedup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple Silicon GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM().to(device)\n",
    "# model = LSTM_Dropout().to(device)\n",
    "# model = LSTM_DropoutNorm().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.25)\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "no_improvement = 0\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffd01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm.tqdm(range(100), desc=\"Epoch\", unit=\"epoch\"):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "        loss = criterion(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_mae = 0\n",
    "    val_mse = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "            val_loss += criterion(pred, y).item()\n",
    "\n",
    "            # show MAE and MSE with unnormalized data\n",
    "            pred = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "            y = y * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "            val_mae += nn.L1Loss()(pred, y).item()\n",
    "            val_mse += nn.MSELoss()(pred, y).item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_mae /= len(val_dataloader)\n",
    "    val_mse /= len(val_dataloader)\n",
    "    scheduler.step()\n",
    "    # scheduler.step(val_loss)\n",
    "\n",
    "    tqdm.tqdm.write(f\"Epoch {epoch:03d} | Learning rate {optimizer.param_groups[0]['lr']:.6f} | train normalized MSE {train_loss:8.4f} | val normalized MSE {val_loss:8.4f}, | val MAE {val_mae:8.4f} | val MSE {val_mse:8.4f}\")\n",
    "    if val_loss < best_val_loss - 1e-3:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        if no_improvement >= early_stopping_patience:\n",
    "            print(\"Early stop!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290b7b5",
   "metadata": {},
   "source": [
    "# Training Loop (Iterate over different scale values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25927804",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for scale in [7.0, 6.0, 5.0, 4.0, 3.0, 2.0]:\n",
    "    print(f\"\\n--- Testing scale: {scale} ---\\n\")\n",
    "\n",
    "    torch.manual_seed(251)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    N = len(train_data)\n",
    "    val_size = int(0.1 * N)\n",
    "    train_size = N - val_size\n",
    "\n",
    "    train_dataset = TrajectoryDatasetTrain(train_data[:train_size], scale=scale, augment=True)\n",
    "    val_dataset = TrajectoryDatasetTrain(train_data[train_size:], scale=scale, augment=False)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "\n",
    "    model = LSTM().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.25)\n",
    "    early_stopping_patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(100), desc=\"Epoch\", unit=\"epoch\"):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "            loss = criterion(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_mae = 0\n",
    "        val_mse = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                batch = batch.to(device)\n",
    "                pred = model(batch)\n",
    "                y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "                val_loss += criterion(pred, y).item()\n",
    "\n",
    "                # Denormalize\n",
    "                pred = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "                y = y * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "                val_mae += nn.L1Loss()(pred, y).item()\n",
    "                val_mse += nn.MSELoss()(pred, y).item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_mae /= len(val_dataloader)\n",
    "        val_mse /= len(val_dataloader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        tqdm.tqdm.write(f\"Epoch {epoch:03d} | LR {optimizer.param_groups[0]['lr']:.6f} | Train MSE {train_loss:.4f} | Val MSE {val_loss:.4f} | MAE {val_mae:.4f} | MSE {val_mse:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-3:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvement = 0\n",
    "            torch.save(model.state_dict(), f\"best_model_scale_{scale}.pt\")\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    results.append((scale, best_val_loss, val_mae, val_mse))\n",
    "\n",
    "# Provide summary of all scale values\n",
    "print(\"\\nSummary of results:\")\n",
    "for scale, loss, mae, mse in results:\n",
    "    print(f\"Scale {scale}: Best Val MSE = {loss:.4f}, MAE = {mae:.4f}, MSE = {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41d0003",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601530f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TrajectoryDatasetTest(test_data, scale=scale)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
    "                         collate_fn=lambda xs: Batch.from_data_list(xs))\n",
    "\n",
    "best_model = torch.load(\"/content/best_model.pt\") # change filename based on model\n",
    "model = LSTM().to(device)\n",
    "# model = LSTM_Dropout.to(device)\n",
    "# model = LSTM_DropoutNorm.to(device)\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "pred_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        pred_norm = model(batch)\n",
    "\n",
    "        # Reshape the prediction to (N, 60, 2)\n",
    "        pred = pred_norm * batch.scale.view(-1,1,1) + batch.origin.unsqueeze(1)\n",
    "        pred_list.append(pred.cpu().numpy())\n",
    "pred_list = np.concatenate(pred_list, axis=0)  # (N,60,2)\n",
    "pred_output = pred_list.reshape(-1, 2)  # (N*60, 2)\n",
    "output_df = pd.DataFrame(pred_output, columns=['x', 'y'])\n",
    "output_df.index.name = 'index'\n",
    "output_df.to_csv('submission.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
